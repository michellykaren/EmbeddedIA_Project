{"cells":[{"cell_type":"markdown","metadata":{"id":"45QfWaJjXpDb"},"source":["**Data augmentation for Esca dataset**\n","\n","---\n","\n"]},{"cell_type":"markdown","metadata":{"id":"6FwbeitX6aeb"},"source":["# STEP 1 - Download dataset from Mendeley repository"]},{"cell_type":"code","execution_count":1,"metadata":{"id":"SBeO86k1XoWM"},"outputs":[{"name":"stderr","output_type":"stream","text":["'wget' n'est pas reconnu en tant que commande interne\n","ou externe, un programme ex�cutable ou un fichier de commandes.\n","'ls' n'est pas reconnu en tant que commande interne\n","ou externe, un programme ex�cutable ou un fichier de commandes.\n","'unzip' n'est pas reconnu en tant que commande interne\n","ou externe, un programme ex�cutable ou un fichier de commandes.\n","'ls' n'est pas reconnu en tant que commande interne\n","ou externe, un programme ex�cutable ou un fichier de commandes.\n"]}],"source":["dataset_name = \"esca_dataset\"\n","# Url to repo (repo temporary saved in Google Drive but intended to Mendeley repo)\n","dataset_url =  \"https://prod-dcd-datasets-cache-zipfiles.s3.eu-west-1.amazonaws.com/89cnxc58kj-1.zip\"   # Google Drive -> to change with Mendely Link\n","# Trick to use wget with gDrive: use 'https://docs.google.com/uc?export=download&id=FILEID' \n","# where FILEID is extracted from the virtual link provided from Google drive\n","dataset_url4wget = \"https://docs.google.com/uc?export=download&id=1qO997Wy5drvRpVbAOCL20w82FEGiDpmV\"\n","# Download the archive directly from url\n","!wget -r --no-check-certificate \"$dataset_url\" -O $dataset_name\".zip\"\n","!ls\n","# Unzip data\n","!unzip  $dataset_name\".zip\"\n","!ls"]},{"cell_type":"markdown","metadata":{"id":"7PfslEEz6kgQ"},"source":["# STEP 2 - Data augmentation"]},{"cell_type":"code","execution_count":2,"metadata":{"id":"-tVvWZ1N-xoN"},"outputs":[],"source":["# Input pameters choosen by user\n","# transformation_array -> User can choose what transformations apply simply comment/uncomment (#) the row corresponding to the transformation name.\n","# enable_show -> User can choose to display (True) or none (False) the transformations applied to original images. Enabling this option, the execution speed will be reduced.\n","\n","transformation_array = [\n","                        \"horizontalFlip\",\n","                        \"verticalFlip\", \n","                        \"rotation\", \n","                        \"widthShift\", \n","                        \"heightShift\",  \n","                        \"shearRange\",\n","                        \"zoom\", \n","                        \"blur\",\n","                        \"brightness\", \n","                        \"contrast\",\n","                        \"saturation\",\n","                        \"hue\",\n","                        \"gamma\"\n","                        ];\n","enable_show = False;"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"7OTi6-txCYc5"},"outputs":[],"source":["# The new dataset 'augmented_esca_dataset' will be created.\n","# This dataset contains the augmented images create by the ImageGenerator class and the orginal images, \n","# in order to obtain an expanded version of the orginal dataset ready-to-use\n","\n","from keras.preprocessing.image import ImageDataGenerator, array_to_img, img_to_array, load_img \n","import tensorflow as tf\n","import os\n","from numpy import expand_dims\n","import cv2\n","import matplotlib.pyplot as plt\n","from pathlib import Path\n","\n","\n","\n","def blur(img):\n","    return (cv2.blur(img,(30,30)))\n","\n","def horizontal_flip(img):\n","    return (tf.image.flip_left_right(img))\n","\n","def vertical_flip(img):\n","    return (tf.image.flip_up_down(img))\n"," \n","def contrast(img):\n","    return (tf.image.adjust_contrast(img, 0.5))\n","\n","def saturation(img):\n","    return (tf.image.adjust_saturation(img, 3))\n","\n","def hue(img):\n","    return (tf.image.adjust_hue(img, 0.1)) \n","\n","def gamma(img):\n","    return (tf.image.adjust_gamma(img, 2))\n","\n","\n","\n","new_dataset = 'augmented_esca_dataset'\n","classes = ['esca', 'healthy']\n","for class_tag in classes:\n","  input_path = '/content/' + dataset_name + '/' + class_tag + '/'\n","  output_path = '/content/' + dataset_name + '/' + new_dataset + '/' + class_tag + '/'\n","  print(input_path)\n","  print(output_path)\n","  # TMP\n","  !rm -rf $output_path\n","  # END TMP\n","  try:\n","    if not os.path.exists(output_path):\n","      os.makedirs(output_path)\n","  except OSError:\n","      print (\"Creation of the directory %s failed\\n\\n\" % output_path)\n","  else:\n","      print (\"Successfully created the directory %s\\n\\n\" % output_path)\n","\n","  for filename in os.listdir(input_path):\n","    if filename.endswith(\".jpg\"):\n","      # Copy the original image in the new dataset\n","      original_file_path = input_path + filename\n","      original_newname_file_path = output_path + Path(filename).stem + \"_original.jpg\"\n","      %cp $original_file_path $original_newname_file_path\n","      # Initialising the ImageDataGenerator class. \n","      # We will pass in the augmentation parameters in the constructor. \n","      for transformation in transformation_array:\n","        if transformation == \"horizontalFlip\":\n","              #datagen = ImageDataGenerator(horizontal_flip = True)                 # for random flip\n","              datagen = ImageDataGenerator(preprocessing_function=horizontal_flip)  # all imgs flipped\n","        elif transformation == \"verticalFlip\":\n","              #datagen = ImageDataGenerator(vertical_flip = True)                   # for random flip\n","              datagen = ImageDataGenerator(preprocessing_function=vertical_flip)    # all imgs flipped\n","        elif transformation == \"rotation\":\n","              datagen = ImageDataGenerator(rotation_range = 40, fill_mode='nearest') \n","        elif transformation == \"widthShift\":\n","              datagen = ImageDataGenerator(width_shift_range = 0.2, fill_mode='nearest')\n","        elif transformation == \"heightShift\":\n","              datagen = ImageDataGenerator(height_shift_range = 0.2, fill_mode='nearest')         \n","        elif transformation == \"shearRange\":\n","              datagen = ImageDataGenerator(shear_range = 0.2)   \n","        elif transformation == \"zoom\":\n","              datagen = ImageDataGenerator(zoom_range = [0.5, 1.0])\n","        elif transformation == \"blur\":\n","              datagen = ImageDataGenerator(preprocessing_function=blur)        \n","        elif transformation == \"brightness\":\n","              #Values less than 1.0 darken the image, e.g. [0.5, 1.0], \n","              #whereas values larger than 1.0 brighten the image, e.g. [1.0, 1.5], \n","              #where 1.0 has no effect on brightness.\n","              datagen = ImageDataGenerator(brightness_range = [1.1, 1.5])\n","        elif transformation == \"contrast\": \n","              datagen = ImageDataGenerator(preprocessing_function=contrast)\n","        elif transformation == \"saturation\": \n","              datagen = ImageDataGenerator(preprocessing_function=saturation)      \n","        elif transformation == \"hue\": \n","              datagen = ImageDataGenerator(preprocessing_function=hue)    \n","        elif transformation == \"gamma\": \n","              datagen = ImageDataGenerator(preprocessing_function=gamma)      \n","\n","        # Loading a sample image \n","        img = load_img(input_path + filename) \n","        # Converting the input sample image to an array \n","        data = img_to_array(img) \n","        # Reshaping the input image expand dimension to one sample\n","        samples = expand_dims(data, 0) \n","        # Plot original image\n","        print(\"Original image:\")\n","        print(filename)\n","        if enable_show:\n","          plt.imshow(img)\n","          plt.show()\n","          print(\"\\n\\n\")\n","\n","        # Generating and saving n_augmented_images augmented samples\n","        print(\"Apply \" + transformation + \".\")\n","        # prepare iterator\n","        it = datagen.flow(samples, batch_size = 1, \n","                    save_to_dir = output_path, \n","                    save_prefix = Path(filename).stem + \"_\" + transformation,\n","                    save_format ='jpg')\n","        batch = it.next()\n","        # Plot trasnformed image\n","        image = batch[0].astype('uint8')\n","        if enable_show:\n","          print(\"Transformed image:\")\n","          plt.imshow(image)\n","          plt.show()\n","        print(\"\\n\\n\")\n","\n","print(\"Done!\\n\\n\")"]},{"cell_type":"markdown","metadata":{"id":"c3YCs9jhyQe2"},"source":["# [Opzional STEP] - Visualize some images generated from data augmentation"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"m2eYvQO6vw8Z"},"outputs":[],"source":["# Visualize N images with data aumentation applied, \n","# where N = n_images_shown can be chosen by the user.\n","\n","import os\n","import glob\n","from numpy import expand_dims\n","import cv2\n","import matplotlib.pyplot as plt\n","from pathlib import Path\n","\n","\n","n_images_shown = 4              # This parameter can be modified by the user\n","\n","for class_tag in classes:\n","  input_path = '/content/' + dataset_name + '/' + class_tag + '/'\n","  output_path = '/content/' + dataset_name + '/' + new_dataset + '/' + class_tag + '/'\n","\n","  counter = 0\n","  for filename in os.listdir(input_path):\n","    if filename.endswith(\".jpg\"):\n","\n","      if counter > n_images_shown:\n","        break\n","\n","      # Show the obtained AUGMENTATED IMAGES (plus ORIGINAL IMAGE) for the specific orginal image\n","      print(filename)\n","      transformation_array_size = len(transformation_array)\n","      # Define subplot\n","      fig = plt.figure(figsize=(30,20))\n","      columns = 3\n","      rows = transformation_array_size / columns + 1  \n","      # Sort images by creation date to obtain \"original\" as first image\n","      files_sorted = list(filter(os.path.isfile, glob.glob(output_path + Path(filename).stem + \"*.jpg\")))\n","      files_sorted.sort(key=lambda x: os.path.getmtime(x))\n","      #print(files_sorted)\n","      index = 0\n","      for filename_out in files_sorted:\n","        #print(filename_out)\n","        # Load image\n","        aug_img = load_img(filename_out) \n","        # Converting the input sample image to an array \n","        data = img_to_array(aug_img) \n","        # Reshaping the input image expand dimension to one sample\n","        samples = expand_dims(data, 0) \n","        # Plot augmented image\n","        ax1 = fig.add_subplot(rows, columns, index + 1)\n","        ax1.title.set_text((Path(filename_out).stem).split('_')[3])\n","        plt.imshow(aug_img)\n","        index = index + 1\n","      print(\"Augmented images:\")\n","      plt.show()\n","      print(\"\\n\\n\")\n","    counter = counter + 1\n","    \n"]},{"cell_type":"markdown","metadata":{"id":"_31hAzIhzGn6"},"source":["# Step 3 - Save augmented dataset"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"nmIV6Ac5X8Rp"},"outputs":[],"source":["#@title Save your data to Google drive\n","filename = \"augmented_esca_dataset\" #@param {type:\"string\"}\n","folders_to_save = \"/content/esca_dataset/augmented_esca_dataset\" #@param {type:\"string\"}\n","\n","from google.colab import files\n","from google.colab import auth\n","from googleapiclient.http import MediaFileUpload\n","from googleapiclient.discovery import build\n","\n","\n","def save_file_to_drive(name, path):\n","    file_metadata = {\n","    'name': name,\n","    'mimeType': 'application/octet-stream'\n","    }\n","    media = MediaFileUpload(path, \n","                  mimetype='application/octet-stream',\n","                  resumable=True)\n","    created = drive_service.files().create(body=file_metadata, media_body=media, fields='id').execute()\n","    print('File ID: {}'.format(created.get('id')))\n","    return created\n","\n","\n","# Create archive\n","extension_zip = \".zip\"\n","zip_file = filename + extension_zip\n","!zip -r $zip_file {folders_to_save}\n","\n","# Save archive to your Google Drive\n","auth.authenticate_user()\n","drive_service = build('drive', 'v3')\n","destination_name = zip_file\n","path_to_file = zip_file\n","save_file_to_drive(destination_name, path_to_file)\n","!ls\n","\n","print(\"Done!\")\n"]},{"cell_type":"markdown","metadata":{"id":"30Xh5aF5-8FP"},"source":["# BACKUP"]},{"cell_type":"markdown","metadata":{"id":"b_N9e-4r_S8v"},"source":["- EXTRACT DATA FROM ARCHIVE\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"UFpQHsXKoLN8"},"outputs":[],"source":["'''\n","import zipfile\n","from google.colab import drive\n","\n","drive.mount('/content/drive/')\n","!ls\n","\n","zip_ref = zipfile.ZipFile(\"/content/drive/My Drive/Colab Notebooks/esca_dataset.zip\", 'r')\n","zip_ref.extractall(\"/content/drive/My Drive/Colab Notebooks/esca_dataset\")\n","zip_ref.close()\n","'''"]},{"cell_type":"markdown","metadata":{"id":"6JwDM2o9lTQ9"},"source":["- ARCHIVE DATA AND SAVE ZIP FILE TO YOUR PC"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"eszRBrPKlLUI"},"outputs":[],"source":["'''\n","# An archive file of the augmented dataset will be created \n","# and automatically downloaded to the path selected by the user in the browse window that will appear at the end of the script\n","\n","from google.colab import files\n","import requests\n","from multiprocessing.pool import ThreadPool\n","from pathlib import Path\n","\n","\n","# Create archive\n","data_folder_tozip = \"/content/\" + dataset_name + \"/\" + new_dataset\n","archive_name = new_dataset + \".zip\"\n","print(data_folder_tozip)\n","\n","#!zip -r $archive_name $data_folder_tozip\n","#files.download(archive_name)\n","\n","# We have a large archive, so we create a split zip archive, that is multiple archives of the same folder (each part of 100 MB)\n","!zip -r -s 100m \"archive.zip\" $data_folder_tozip\n","\n","# Download the split archive, so we have multiple archives on the chosen path \n","def download_archive(archive):\n","  print(\"Downloading: \",archive)\n","  files.download(archive)\n"," \n","archive_array = []\n","for path in Path('/content/').rglob('archive.z*'):\n","    print(path.name)\n","    string = path.name\n","    archive_array.append(string)\n","print(archive_array)\n","print(len(archive_array))\n","\n","# Run multiple threads to download the multiple archives in parallel\n","ThreadPool(len(archive_array)).imap_unordered(download_archive, archive_array)\n","\n","\n","# NOTE: After download the multiple archives, the use must reconstruct the archive:\n","# - For Linux users:\n","# To unzip the file, first convert a split archive to a single-file archive:\n","# zip -s 0 archive.zip --out unsplit.zip\n","# Then you can unzip the \"unsplit\" file\n","# unzip unsplit.zip\n","#\n","# - For Windows:\n","# \n","'''"]}],"metadata":{"colab":{"authorship_tag":"ABX9TyM3vVpOYU2pdMyNdBpSF2Vk","collapsed_sections":[],"name":"esca_dataset-augmentation.ipynb","provenance":[]},"kernelspec":{"display_name":"Python 3.9.7 ('base')","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.7"},"vscode":{"interpreter":{"hash":"f82454df3ab4669350e470cecfad51160e6fdff8e76eafd19d8880dd92d922a3"}}},"nbformat":4,"nbformat_minor":0}
